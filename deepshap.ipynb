{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be7e1659-6868-4bd2-87ee-ecac0250e8db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ldap/william/anaconda3/envs/pytorch_1_12/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torchvision import models, transforms\n",
    "import torch.nn as nn\n",
    "from pytorch_grad_cam import GradCAM, GradCAMPlusPlus\n",
    "from pytorch_grad_cam import GuidedBackpropReLUModel\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image, \\\n",
    "    deprocess_image, \\\n",
    "    preprocess_image\n",
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f547fcb-28f2-421b-9d37-53f9230894e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Loader\n",
    "class CSVDataset(Dataset):\n",
    "    def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n",
    "        self.img_labels = pd.read_csv(annotations_file, delimiter=' ', header=None)\n",
    "        self.class_num = 15\n",
    "        self.targets = self.img_labels.iloc[:, 1]\n",
    "        # for i in range(len(self.targets)):\n",
    "        #     if self.targets[i] > self.class_num:\n",
    "        #         self.targets[i] = 0\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
    "        # image = Image.open(img_path)\n",
    "        # if image.mode != 'RGB':\n",
    "        #     image = image.convert('RGB')\n",
    "        label = self.img_labels.iloc[idx, 1]\n",
    "        # image = np.array(image)\n",
    "        if label > self.class_num:\n",
    "            label = 0\n",
    "        \n",
    "        x = cv2.imread(img_path)\n",
    "        # x = x[:, :, ::-1]\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image=x)['image']\n",
    "        return image, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "SHAP_transforms = A.Compose([\n",
    "        A.Resize(256, 256),\n",
    "        A.CenterCrop(224, 224),\n",
    "        A.Normalize(mean=[0.2527, 0.3085, 0.3082], std=[0.1234, 0.1629, 0.1564]),\n",
    "        # A.Normalize(mean=[0.2266, 0.2886, 0.2763], std=[0.1125, 0.1538, 0.1363]),\n",
    "        ToTensorV2()\n",
    "    ])\n",
    "train_set = CSVDataset('/home/ldap/william/private/chrislin/AUO_Data_811_DA/train_Open_DA_ordered.csv', '/home/ldap/william/private/chrislin/AUO_Data_811_DA', SHAP_transforms)\n",
    "test_set = CSVDataset('/hcds_vol/private/chrislin/20230830_C101/test_0830.csv', '/hcds_vol/private/chrislin/20230830_C101/', SHAP_transforms)\n",
    "# train_set = CSVDataset('/home/ldap/william/private/NCU/william/all_csv/chrislin/train.csv', '/home/ldap/william/private/chrislin/AUO_Data_811_DA', SHAP_transforms)\n",
    "# test_set = CSVDataset('/home/ldap/william/private/NCU/william/all_csv/chrislin/test.csv', '/hcds_vol/private/chrislin/20230830_C101/', SHAP_transforms)\n",
    "# test_set = CSVDataset('/hcds_vol/private/chrislin/AUO_Data_811_DA/val_Open_ordered.csv', '/hcds_vol/private/chrislin/AUO_Data_811_DA/', SHAP_transforms)\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=50, shuffle=True, num_workers=24, pin_memory=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=50, shuffle=False, num_workers=24, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8581969d-fdc4-4daa-97f2-f9a779ec860a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "Enable dynamical reprensetation expansion!\n",
      "16\n",
      "cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataParallel(\n",
       "  (module): BasicNet(\n",
       "    (convnets): ModuleList(\n",
       "      (0): ResNet(\n",
       "        (conv1): Sequential(\n",
       "          (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (2): ReLU(inplace=True)\n",
       "          (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "        )\n",
       "        (layer1): Sequential(\n",
       "          (0): BasicBlock(\n",
       "            (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (1): BasicBlock(\n",
       "            (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (layer2): Sequential(\n",
       "          (0): BasicBlock(\n",
       "            (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "            (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (downsample): Sequential(\n",
       "              (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "              (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (1): BasicBlock(\n",
       "            (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (layer3): Sequential(\n",
       "          (0): BasicBlock(\n",
       "            (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (downsample): Sequential(\n",
       "              (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "              (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (1): BasicBlock(\n",
       "            (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (layer4): Sequential(\n",
       "          (0): BasicBlock(\n",
       "            (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "            (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (downsample): Sequential(\n",
       "              (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "              (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (1): BasicBlock(\n",
       "            (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "            (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "      )\n",
       "    )\n",
       "    (avgpool2): AvgPool2d(kernel_size=1, stride=1, padding=0)\n",
       "    (classifier): Linear(in_features=512, out_features=16, bias=False)\n",
       "    (aux_classifier): Linear(in_features=512, out_features=17, bias=False)\n",
       "    (adv_classifier): Linear(in_features=512, out_features=2, bias=False)\n",
       "    (Convmask): Conv2d(512, 1, kernel_size=(1, 1), stride=(2, 2))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "# Load Model\n",
    "\n",
    "repo_name = 'DER'\n",
    "base_dir = os.path.realpath(\".\")[:os.path.realpath(\".\").index(repo_name) + len(repo_name)]\n",
    "sys.path.insert(0, base_dir)\n",
    "\n",
    "task_id = 0\n",
    "\n",
    "import yaml\n",
    "from inclearn.convnet import network\n",
    "from torch.nn import DataParallel\n",
    "from easydict import EasyDict as edict\n",
    "\n",
    "    \n",
    "# config_file = os.path.join(w_dir, \"1.yaml\")\n",
    "config_file = \"./configs/1.yaml\"\n",
    "with open(config_file, 'r') as stream:\n",
    "    try:\n",
    "        config = yaml.safe_load(stream)\n",
    "    except yaml.YAMLError as exc:\n",
    "        print(exc)\n",
    "\n",
    "# device = \"cuda:0\"\n",
    "device = \"cpu\"\n",
    "\n",
    "cfg = edict(config)\n",
    "model = network.BasicNet(\n",
    "    cfg[\"convnet\"],\n",
    "    cfg = cfg,\n",
    "    nf = cfg[\"channel\"],\n",
    "    device = device,\n",
    "    use_bias = cfg[\"use_bias\"],\n",
    "    dataset = cfg[\"dataset\"],\n",
    ")\n",
    "parallel_model = DataParallel(model)\n",
    "\n",
    "total_classes = 28\n",
    "increments = []\n",
    "increments.append(cfg[\"start_class\"])\n",
    "for _ in range((total_classes - cfg[\"start_class\"]) // cfg[\"increment\"]):\n",
    "    increments.append(cfg[\"increment\"])\n",
    "\n",
    "for i in range(task_id+1):\n",
    "    model.add_classes(increments[i])\n",
    "    model.task_size = increments[i]\n",
    "\n",
    "if task_id == 0:\n",
    "    state_dict = torch.load(f'./ckpts/step{task_id}.ckpt')\n",
    "else:\n",
    "    state_dict = torch.load(f'./ckpts/decouple_step{task_id}.ckpt')\n",
    "\n",
    "parallel_model.cuda()\n",
    "# parallel_model.to(\"cpu\")\n",
    "parallel_model.load_state_dict(state_dict)\n",
    "parallel_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3313802c-f99e-43e5-baf6-033886532079",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 得到模型的prediction\n",
    "class ResnetPrediction(torch.nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super(ResnetPrediction, self).__init__()\n",
    "        self.model = model\n",
    "    def forward(self, x):\n",
    "        return self.model(x)['feature']\n",
    "        \n",
    "        # return self.model(x)['feature']\n",
    "        # return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a7016e4c-7b0d-4798-83ad-fc2716099afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import shap\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# model\n",
    "M = ResnetPrediction(parallel_model.module.cuda()).cuda()\n",
    "train_feature = []\n",
    "test_feature = []\n",
    "with torch.no_grad():\n",
    "    for batch_cnt, train_data in enumerate(train_loader):\n",
    "        train_images, _ = train_data\n",
    "        train_images = train_images.cuda()\n",
    "        if batch_cnt == 0:\n",
    "            train_feature = M(train_images)\n",
    "        else:\n",
    "            train_feature = torch.cat([train_feature,M(train_images)])\n",
    "        if batch_cnt == 19:\n",
    "            break\n",
    "\n",
    "explainer = shap.DeepExplainer(M.model.classifier, train_feature)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "836d5b09-7498-436a-85fe-f24f005d733a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for batch_cnt, test_data in enumerate(test_loader):\n",
    "        test_images, _ = test_data\n",
    "        test_images = test_images.cuda()\n",
    "        if batch_cnt == 0:\n",
    "            test_feature = M(test_images)\n",
    "        else:\n",
    "            test_feature = torch.cat([test_feature,M(test_images)])\n",
    "shap_value = explainer.shap_values(test_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b5007594-14e8-4f51-9848-9f5e3a5713c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "10269\n",
      "512\n"
     ]
    }
   ],
   "source": [
    "print(len(shap_value))\n",
    "print(len(shap_value[0]))\n",
    "print(len(shap_value[0][0]))\n",
    "# print(shap_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3947443d-b3cd-4a3b-a8c2-560e69a59065",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 800x950 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = test_feature.cpu().detach().numpy()\n",
    "shap_value = np.array(shap_value)\n",
    "for i in range(16):\n",
    "    shap.summary_plot(shap_value[i],data,max_display=20,plot_type=\"bar\",show=False)\n",
    "    name = 'class' + str(i) + '.jpg'\n",
    "    plt.savefig('/home/ldap/william/private/NCU/william/DER/exps/opena_811_224_DA_DCL75_Jigsaw_7x7_Mixup_20230830_200/shap_images/test/'+name)\n",
    "    plt.clf()\n",
    "# shap.plots.beeswarm(shap_values,max_display=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ee74d55e-e250-43da-8c9b-b61e975a1859",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n"
     ]
    }
   ],
   "source": [
    "train_shap_value = explainer.shap_values(train_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c5af859e-0fa9-4b83-9520-ce5ffb827e82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 800x950 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_data = train_feature.cpu().detach().numpy()\n",
    "train_shap_value = np.array(train_shap_value)\n",
    "for i in range(16):\n",
    "    shap.summary_plot(train_shap_value[i],train_data,max_display=20,plot_type=\"bar\",show=False)\n",
    "    name = 'class' + str(i) + '.jpg'\n",
    "    plt.savefig('/home/ldap/william/private/NCU/william/DER/exps/opena_811_224_DA_DCL75_Jigsaw_7x7_Mixup_20230830_200/shap_images/train/'+name)\n",
    "    plt.clf()\n",
    "# shap.summary_plot(train_shap_value[3],train_data,max_display=30,plot_type=\"bar\",show=False)\n",
    "#plt.savefig('/home/ldap/william/private/NCU/william/DER/exps/opena_811_224_DA_DCL75_Jigsaw_7x7_Mixup_20230830_200/shap_images/train_shap.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e5b323-285f-4308-a065-c474bc25a0ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_1_12",
   "language": "python",
   "name": "pytorch_1_12"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
